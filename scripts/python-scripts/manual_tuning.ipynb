{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import all packages and library\n",
    "\n",
    "# Import package to scan hyperparameter\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Import package to reprocess the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Import properties from keras\n",
    "from keras import models\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "\n",
    "# Import keras items\n",
    "from keras.optimizers import Adam, Adadelta, SGD\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the data and reprocess them\n",
    "\n",
    "# Get the reprocessed data from .npy file\n",
    "x_train = np.load('../r-scripts/getting-data-current/data-sets/x_train.npy')\n",
    "y_train = np.load('../r-scripts/getting-data-current/data-sets/y_train.npy')\n",
    "\n",
    "x_dev = np.load('../r-scripts/getting-data-current/data-sets/x_val.npy')\n",
    "y_dev = np.load('../r-scripts/getting-data-current/data-sets/y_val.npy')\n",
    "\n",
    "x_test = np.load('../r-scripts/getting-data-current/data-sets/x_test.npy')\n",
    "y_test = np.load('../r-scripts/getting-data-current/data-sets/y_test.npy')\n",
    "\n",
    "# This Section is used to shuffle the data\n",
    "\n",
    "# This Section is used to shuffle the data\n",
    "\n",
    "# Aggregates elements\n",
    "data_training = list(zip(x_train, y_train))\n",
    "data_development = list(zip(x_dev, y_dev))\n",
    "data_testing = list(zip(x_test, y_test))\n",
    "\n",
    "# Shuffle the aggragated element on the list\n",
    "random.shuffle(data_training)\n",
    "random.shuffle(data_development)\n",
    "random.shuffle(data_testing)\n",
    "\n",
    "# Combine data training dan data development become one list of data train\n",
    "\n",
    "data_train = data_training + data_development\n",
    "\n",
    "# Split the shuffled data\n",
    "x_train, y_train = zip(*data_train)\n",
    "x_test, y_test = zip(*data_testing)\n",
    "\n",
    "# Unpack the tuples\n",
    "x_train = np.array(list(x_train))\n",
    "y_train = np.array(list(y_train))\n",
    "x_test = np.array(list(x_test))\n",
    "y_test = np.array(list(y_test))\n",
    "\n",
    "# Reshape the datasets\n",
    "x_train = x_train.reshape(615, 4034 * 20)\n",
    "x_test = x_test.reshape(150, 4034 * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and function\n",
    "\n",
    "# Define the model base\n",
    "def build_fc_model(# Hyperparameters as parts of model designs (building blocks)\n",
    "\t               input_num_hidden_units = 2,\n",
    "                   num_hidden_layers = [0],\n",
    "                   activation_function = 'relu',\n",
    "\n",
    "                   # Hyperparameters as part of optimization and regularization of the models\n",
    "                   l2_rate = 0.01,\n",
    "                   input_dropout_rates = 0.5,\n",
    "                   dropout_rates = 0.5,\n",
    "                   optim_methods = 'Adam',\n",
    "                   batch_norm = \"yes\"\n",
    "                   ):\n",
    "\n",
    "\t# Add the input layer\n",
    "    model = models.Sequential()\n",
    "    model.add(Dense(input_num_hidden_units,\n",
    "              activation = 'relu',\n",
    "              kernel_regularizer = regularizers.l2(l2_rate),\n",
    "              kernel_initializer=initializers.glorot_uniform(seed=3),\n",
    "              input_dim = x_train.shape[1]))\n",
    "    model.add(Dropout(input_dropout_rates))\n",
    "\n",
    "    # Add the hidden layers\n",
    "    for num in range(len(num_hidden_layers)):\n",
    "        if num_hidden_layers[num] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            model.add(Dense(num_hidden_layers[num]))\n",
    "\n",
    "            # Add batch normaization before adding the activation layers\n",
    "            if batch_norm == \"yes\":\n",
    "            \tmodel.add(BatchNormalization())\n",
    "            else:\n",
    "            \tcontinue\n",
    "\n",
    "            model.add(Activation(activation_function))\n",
    "            model.add(Dropout(dropout_rates))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1,\n",
    "              activation = 'sigmoid'))\n",
    "\n",
    "    # Compile the model defined\n",
    "    model.compile(optimizer = optim_methods,\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['acc'])\n",
    "\n",
    "    # Print the summary of the model\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_113 (Dense)            (None, 3)                 242043    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 3)                 6         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 242,057\n",
      "Trainable params: 242,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "   Epochs  Batch Size   Acc\n",
      "0    40.0        16.0  0.74\n"
     ]
    }
   ],
   "source": [
    "# Predict the results of hyperparamaters tuning\n",
    "\n",
    "# Define function to fit the model\n",
    "def train_fc_model(batch_sizes = None, num_epochs = None):\n",
    "      model.fit(x = x_train,\n",
    "                y = y_train,\n",
    "                batch_size = batch_sizes,\n",
    "                epochs = num_epochs,\n",
    "                verbose = 0,\n",
    "                shuffle = 1)\n",
    "\n",
    "# Define the function to calculate sensitivity and specificity\n",
    "def sensitivity_specificity(predictions, y_test, mode='binary'):\n",
    "    if mode == 'binary':\n",
    "        # Determine positive class predictions\n",
    "        index = predictions > 0.5\n",
    "        predictions = np.zeros(predictions.shape)\n",
    "        predictions[index] = 1\n",
    "        # No need to modify y_test since it consists of zeros and ones already\n",
    "    else:\n",
    "        y_test = y_test\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # In the binary classification case as we create, we can extract tn, fp, fn, tp as follows\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions, labels = [0, 1]).ravel()\n",
    "\n",
    "    # Sensitivity = TP / (TP + FN)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "\n",
    "    # Specificity = TN / (TN + FP)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Return sensitivity, specificity, precision\n",
    "    return(sensitivity, specificity, precision)\n",
    "\n",
    "\n",
    "# Define function to evaluate and predict\n",
    "def evaluate_predict_fc_model():\n",
    "  loss, acc = model.evaluate(x_test, y_test, verbose = 0)\n",
    "  prediction = model.predict(x_test)\n",
    "  sensitivity, specificity, precision = sensitivity_specificity(prediction, y_test, mode='binary')\n",
    "  return acc, sensitivity, specificity, precision\n",
    "\n",
    "# make prediction\n",
    "num_epochs = [1]\n",
    "num_batch_size = [4, 8, 16]\n",
    "\n",
    "result_list = []\n",
    "columns_names = ['Epochs', \n",
    "                 'Batch Size', \n",
    "                 'Acc']\n",
    "\n",
    "for i in range(len(num_epochs)):\n",
    "  for j in range(len(num_batch_size)):\n",
    "    model = build_fc_model(# Hyperparameters as parts of model designs (building blocks)\n",
    "\t                       input_num_hidden_units = 3,\n",
    "                           num_hidden_layers = [1, 3],\n",
    "                           activation_function = 'relu',\n",
    "\n",
    "                           # Hyperparameters as part of optimization and regularization of the models\n",
    "                           l2_rate = 0.01,\n",
    "                           input_dropout_rates = 0.5,\n",
    "                           dropout_rates = 0.5,\n",
    "                           optim_methods = 'SGD',\n",
    "                           batch_norm = 'no')\n",
    "    train_fc_model(batch_sizes = num_batch_size[j], num_epochs = num_epochs[i])\n",
    "    acc, sensitivity, specifity, precision = evaluate_predict_fc_model()\n",
    "    result_line = np.array((num_epochs[i],\n",
    "                            num_batch_size[j],\n",
    "                            acc))\n",
    "    result_list.append(result_line[:])\n",
    "    result_array = np.asarray(result_list)\n",
    "    df_results = pd.DataFrame(result_array,\n",
    "                              columns = columns_names)\n",
    "    \n",
    "# df_results.to_csv('df_result_manual_model1.csv')\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
