{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import all packages and library\n",
    "\n",
    "# Import package to scan hyperparameter\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import package to reprocess the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Import properties from keras\n",
    "from keras import models\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import regularizers\n",
    "\n",
    "# Import keras items\n",
    "from keras.optimizers import Adam, Adadelta, SGD\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the data and reprocess them\n",
    "\n",
    "# Get the reprocessed data from .npy file\n",
    "x_train = np.load('../r-scripts/getting-data-current/data-sets/x_train.npy')\n",
    "y_train = np.load('../r-scripts/getting-data-current/data-sets/y_train.npy')\n",
    "\n",
    "x_dev = np.load('../r-scripts/getting-data-current/data-sets/x_val.npy')\n",
    "y_dev = np.load('../r-scripts/getting-data-current/data-sets/y_val.npy')\n",
    "\n",
    "x_test = np.load('../r-scripts/getting-data-current/data-sets/x_test.npy')\n",
    "y_test = np.load('../r-scripts/getting-data-current/data-sets/y_test.npy')\n",
    "\n",
    "# This Section is used to shuffle the data\n",
    "\n",
    "# Aggregates elements\n",
    "data_training = list(zip(x_train, y_train))\n",
    "data_development = list(zip(x_dev, y_dev))\n",
    "data_testing = list(zip(x_test, y_test))\n",
    "\n",
    "# Shuffle the aggragated element on the list\n",
    "random.shuffle(data_training)\n",
    "random.shuffle(data_development)\n",
    "random.shuffle(data_testing)\n",
    "\n",
    "# Combine data training dan data development become one list of data train\n",
    "\n",
    "data_train = data_training + data_development\n",
    "\n",
    "# Split the shuffled data\n",
    "x_train, y_train = zip(*data_train)\n",
    "x_test, y_test = zip(*data_testing)\n",
    "\n",
    "# Unpack the tuples\n",
    "x_train = np.array(list(x_train))\n",
    "y_train = np.array(list(y_train))\n",
    "x_test = np.array(list(x_test))\n",
    "y_test = np.array(list(y_test))\n",
    "\n",
    "# Reshape the datasets\n",
    "x_train = x_train.reshape(615, 4034 * 20)\n",
    "x_test = x_test.reshape(150, 4034 * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and function\n",
    "\n",
    "# Define the model base\n",
    "def build_fc_model(# Hyperparameters as parts of model designs (building blocks)\n",
    "\t                 input_num_hidden_units = 2,\n",
    "                   num_hidden_layers = [0],\n",
    "                   activation_function = 'relu',\n",
    "\n",
    "                   # Hyperparameters as part of optimization and regularization of the models\n",
    "                   l2_rate = 0.001,\n",
    "                   input_dropout_rates = 0.5,\n",
    "                   dropout_rates = 0.5,\n",
    "                   optim_methods = 'Adam',\n",
    "                   batch_norm = \"yes\"\n",
    "                   ):\n",
    "\n",
    "\t# Add the input layer\n",
    "    model = models.Sequential()\n",
    "    model.add(Dense(input_num_hidden_units,\n",
    "              activation = 'relu',\n",
    "              kernel_regularizer = regularizers.l2(l2_rate),\n",
    "              input_dim = x_train.shape[1]))\n",
    "    model.add(Dropout(input_dropout_rates))\n",
    "\n",
    "    # Add the hidden layers\n",
    "    for num in range(len(num_hidden_layers)):\n",
    "        if num_hidden_layers[num] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            model.add(Dense(num_hidden_layers[num]))\n",
    "\n",
    "            # Add batch normaization before adding the activation layers\n",
    "            if batch_norm == \"yes\":\n",
    "            \tmodel.add(BatchNormalization())\n",
    "            else:\n",
    "            \tcontinue\n",
    "\n",
    "            model.add(Activation(activation_function))\n",
    "            model.add(Dropout(dropout_rates))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1,\n",
    "              activation = 'sigmoid'))\n",
    "\n",
    "    # Compile the model defined\n",
    "    model.compile(optimizer = optim_methods,\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['acc'])\n",
    "\n",
    "    # Print the summary of the model\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the model design to KerasClassifier() wrapper -------------------------------\n",
    "\n",
    "model = KerasClassifier(build_fn = build_fc_model,\n",
    "\t                         verbose = 1)\n",
    "\n",
    "# Define the parameters that will be tuned randomly\n",
    "keras_param_options = {\n",
    "                       # Hyperparameters as parts of model designs (building blocks)\n",
    "                       'input_num_hidden_units': [3],\n",
    "                       'num_hidden_layers': [[1, 3]],\n",
    "                       'activation_function': ['relu'],\n",
    "                       # Hyperparameters as part of optimization and regularization of the models\n",
    "                       'optim_methods' : ['SGD'],\n",
    "                       'l2_rate':[0.01],\n",
    "                       'input_dropout_rates': [0.5],\n",
    "                       'dropout_rates': [0.5],\n",
    "                       'batch_norm' : ['no'],\n",
    "                       # Fitting parameters\n",
    "                       'batch_size': [8],\n",
    "                       'epochs': [30],\n",
    "                       'shuffle': [True]\n",
    "                      }\n",
    "\n",
    "# Using RandomizedSearchCV to find the best model randomly\n",
    "random_search = RandomizedSearchCV(model,\n",
    "                                   param_distributions = keras_param_options,\n",
    "                                   return_train_score=True,\n",
    "                                   n_iter = 1,\n",
    "                                   cv = 5,\n",
    "                                   verbose = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate sensitivity and specificity\n",
    "def sensitivity_specificity(predictions, y_test, mode='binary'):\n",
    "    if mode == 'binary':\n",
    "        # Determine positive class predictions\n",
    "        index = predictions > 0.5\n",
    "        predictions = np.zeros(predictions.shape)\n",
    "        predictions[index] = 1\n",
    "        # No need to modify y_test since it consists of zeros and ones already\n",
    "    else:\n",
    "        y_test = y_test\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # In the binary classification case as we create, we can extract tn, fp, fn, tp as follows\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions, labels = [0, 1]).ravel()\n",
    "    \n",
    "    # accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "\n",
    "    # Sensitivity = TP / (TP + FN)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "\n",
    "    # Specificity = TN / (TN + FP)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Return sensitivity, specificity, precision\n",
    "    return(acc, sensitivity, specificity, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 3)                 242043    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 6         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 242,057\n",
      "Trainable params: 242,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "492/492 [==============================] - 2s 4ms/step - loss: 0.7526 - acc: 0.5061\n",
      "Epoch 2/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7502 - acc: 0.4939\n",
      "Epoch 3/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7455 - acc: 0.5813\n",
      "Epoch 4/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7404 - acc: 0.6504\n",
      "Epoch 5/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7294 - acc: 0.6687\n",
      "Epoch 6/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7201 - acc: 0.7053\n",
      "Epoch 7/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7003 - acc: 0.7114\n",
      "Epoch 8/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6855 - acc: 0.7683\n",
      "Epoch 9/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6586 - acc: 0.7866\n",
      "Epoch 10/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6112 - acc: 0.7988\n",
      "Epoch 11/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5874 - acc: 0.8171\n",
      "Epoch 12/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5283 - acc: 0.8537\n",
      "Epoch 13/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4976 - acc: 0.8577\n",
      "Epoch 14/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4643 - acc: 0.8659\n",
      "Epoch 15/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4336 - acc: 0.8679\n",
      "Epoch 16/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3973 - acc: 0.8801\n",
      "Epoch 17/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3573 - acc: 0.9045\n",
      "Epoch 18/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3624 - acc: 0.9024\n",
      "Epoch 19/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3249 - acc: 0.9106\n",
      "Epoch 20/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3180 - acc: 0.9187\n",
      "Epoch 21/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3057 - acc: 0.9187\n",
      "Epoch 22/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2965 - acc: 0.9289\n",
      "Epoch 23/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2882 - acc: 0.9248\n",
      "Epoch 24/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2662 - acc: 0.9390\n",
      "Epoch 25/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2573 - acc: 0.9390\n",
      "Epoch 26/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2740 - acc: 0.9329\n",
      "Epoch 27/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2309 - acc: 0.9492\n",
      "Epoch 28/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2410 - acc: 0.9451\n",
      "Epoch 29/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2505 - acc: 0.9411\n",
      "Epoch 30/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2441 - acc: 0.9431\n",
      "123/123 [==============================] - 0s 3ms/step\n",
      "492/492 [==============================] - 0s 821us/step\n",
      "[CV]  shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu, score=0.6991869918699187, total=  29.2s\n",
      "[CV] shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 3)                 242043    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 3)                 6         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 242,057\n",
      "Trainable params: 242,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "492/492 [==============================] - 2s 5ms/step - loss: 0.7483 - acc: 0.5203\n",
      "Epoch 2/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7077 - acc: 0.5935\n",
      "Epoch 3/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6388 - acc: 0.6707\n",
      "Epoch 4/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6009 - acc: 0.6931\n",
      "Epoch 5/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5510 - acc: 0.7175\n",
      "Epoch 6/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4652 - acc: 0.8150\n",
      "Epoch 7/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4841 - acc: 0.7805\n",
      "Epoch 8/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4402 - acc: 0.8150\n",
      "Epoch 9/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3953 - acc: 0.8313\n",
      "Epoch 10/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4068 - acc: 0.8354\n",
      "Epoch 11/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3779 - acc: 0.8577\n",
      "Epoch 12/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3342 - acc: 0.8801\n",
      "Epoch 13/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3315 - acc: 0.8760\n",
      "Epoch 14/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3313 - acc: 0.8720\n",
      "Epoch 15/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3488 - acc: 0.8557\n",
      "Epoch 16/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3398 - acc: 0.8455\n",
      "Epoch 17/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3160 - acc: 0.8638\n",
      "Epoch 18/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3201 - acc: 0.8780\n",
      "Epoch 19/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3307 - acc: 0.8455\n",
      "Epoch 20/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2982 - acc: 0.8740\n",
      "Epoch 21/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3125 - acc: 0.8720\n",
      "Epoch 22/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2773 - acc: 0.8902\n",
      "Epoch 23/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3163 - acc: 0.8577\n",
      "Epoch 24/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3014 - acc: 0.8638\n",
      "Epoch 25/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3104 - acc: 0.8598\n",
      "Epoch 26/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2972 - acc: 0.8780\n",
      "Epoch 27/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2965 - acc: 0.8659\n",
      "Epoch 28/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2705 - acc: 0.8862\n",
      "Epoch 29/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2945 - acc: 0.8760\n",
      "Epoch 30/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3306 - acc: 0.8415\n",
      "123/123 [==============================] - 0s 3ms/step\n",
      "492/492 [==============================] - 0s 827us/step\n",
      "[CV]  shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu, score=0.691056910811401, total=  29.3s\n",
      "[CV] shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   59.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 3)                 242043    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 6         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 242,057\n",
      "Trainable params: 242,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "492/492 [==============================] - 2s 4ms/step - loss: 0.7516 - acc: 0.5325\n",
      "Epoch 2/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7386 - acc: 0.5813\n",
      "Epoch 3/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7094 - acc: 0.7012\n",
      "Epoch 4/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6689 - acc: 0.6992\n",
      "Epoch 5/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6315 - acc: 0.7541\n",
      "Epoch 6/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5717 - acc: 0.7886\n",
      "Epoch 7/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5197 - acc: 0.8110\n",
      "Epoch 8/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4263 - acc: 0.8679\n",
      "Epoch 9/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4257 - acc: 0.8415\n",
      "Epoch 10/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3963 - acc: 0.8516\n",
      "Epoch 11/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3388 - acc: 0.8984\n",
      "Epoch 12/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3671 - acc: 0.8598\n",
      "Epoch 13/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3540 - acc: 0.8679\n",
      "Epoch 14/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3253 - acc: 0.8821\n",
      "Epoch 15/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3202 - acc: 0.8699\n",
      "Epoch 16/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3293 - acc: 0.8679\n",
      "Epoch 17/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3085 - acc: 0.8801\n",
      "Epoch 18/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3657 - acc: 0.8394\n",
      "Epoch 19/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2978 - acc: 0.8943\n",
      "Epoch 20/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3279 - acc: 0.8557\n",
      "Epoch 21/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3135 - acc: 0.8801\n",
      "Epoch 22/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3066 - acc: 0.8821\n",
      "Epoch 23/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2669 - acc: 0.9045\n",
      "Epoch 24/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3178 - acc: 0.8598\n",
      "Epoch 25/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3282 - acc: 0.8557\n",
      "Epoch 26/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3027 - acc: 0.8760\n",
      "Epoch 27/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2939 - acc: 0.8862\n",
      "Epoch 28/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3202 - acc: 0.8679\n",
      "Epoch 29/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2898 - acc: 0.8841\n",
      "Epoch 30/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2824 - acc: 0.8882\n",
      "123/123 [==============================] - 0s 3ms/step\n",
      "492/492 [==============================] - 0s 817us/step\n",
      "[CV]  shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu, score=0.7723577240618263, total=  27.9s\n",
      "[CV] shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 3)                 242043    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 3)                 6         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 242,057\n",
      "Trainable params: 242,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "492/492 [==============================] - 2s 5ms/step - loss: 0.7526 - acc: 0.5122\n",
      "Epoch 2/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7504 - acc: 0.5102\n",
      "Epoch 3/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7475 - acc: 0.5224\n",
      "Epoch 4/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7436 - acc: 0.5285\n",
      "Epoch 5/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7337 - acc: 0.5752\n",
      "Epoch 6/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7106 - acc: 0.6565\n",
      "Epoch 7/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6643 - acc: 0.6829\n",
      "Epoch 8/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6078 - acc: 0.7276\n",
      "Epoch 9/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5156 - acc: 0.7764\n",
      "Epoch 10/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4694 - acc: 0.8171\n",
      "Epoch 11/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.8313\n",
      "Epoch 12/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3707 - acc: 0.8638\n",
      "Epoch 13/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3769 - acc: 0.8516\n",
      "Epoch 14/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3337 - acc: 0.8821\n",
      "Epoch 15/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3255 - acc: 0.8699\n",
      "Epoch 16/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3389 - acc: 0.8679\n",
      "Epoch 17/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3031 - acc: 0.8943\n",
      "Epoch 18/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3181 - acc: 0.8679\n",
      "Epoch 19/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3046 - acc: 0.8882\n",
      "Epoch 20/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3218 - acc: 0.8577\n",
      "Epoch 21/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2816 - acc: 0.8984\n",
      "Epoch 22/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3174 - acc: 0.8699\n",
      "Epoch 23/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2855 - acc: 0.8902\n",
      "Epoch 24/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2826 - acc: 0.8841\n",
      "Epoch 25/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2831 - acc: 0.8902\n",
      "Epoch 26/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2717 - acc: 0.9024\n",
      "Epoch 27/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3302 - acc: 0.8577\n",
      "Epoch 28/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2969 - acc: 0.8699\n",
      "Epoch 29/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2641 - acc: 0.9024\n",
      "Epoch 30/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2879 - acc: 0.8841\n",
      "123/123 [==============================] - 0s 3ms/step\n",
      "492/492 [==============================] - 0s 774us/step\n",
      "[CV]  shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu, score=0.6178861788617886, total=  28.8s\n",
      "[CV] shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 3)                 242043    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 3)                 6         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 242,057\n",
      "Trainable params: 242,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "492/492 [==============================] - 2s 4ms/step - loss: 0.7526 - acc: 0.5122\n",
      "Epoch 2/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7457 - acc: 0.5203\n",
      "Epoch 3/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7374 - acc: 0.5386\n",
      "Epoch 4/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7262 - acc: 0.5122\n",
      "Epoch 5/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7054 - acc: 0.5732\n",
      "Epoch 6/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6808 - acc: 0.6951\n",
      "Epoch 7/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.6323 - acc: 0.7581\n",
      "Epoch 8/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5876 - acc: 0.8028\n",
      "Epoch 9/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5461 - acc: 0.8435\n",
      "Epoch 10/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5048 - acc: 0.8598\n",
      "Epoch 11/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4612 - acc: 0.8943\n",
      "Epoch 12/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4112 - acc: 0.9024\n",
      "Epoch 13/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3760 - acc: 0.9187\n",
      "Epoch 14/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3505 - acc: 0.9167\n",
      "Epoch 15/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3289 - acc: 0.9289\n",
      "Epoch 16/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3236 - acc: 0.9228\n",
      "Epoch 17/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3016 - acc: 0.9248\n",
      "Epoch 18/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2695 - acc: 0.9411\n",
      "Epoch 19/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2710 - acc: 0.9370\n",
      "Epoch 20/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.3013 - acc: 0.9228\n",
      "Epoch 21/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2783 - acc: 0.9309\n",
      "Epoch 22/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2714 - acc: 0.9329\n",
      "Epoch 23/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2455 - acc: 0.9451\n",
      "Epoch 24/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2451 - acc: 0.9431\n",
      "Epoch 25/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2684 - acc: 0.9309\n",
      "Epoch 26/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2553 - acc: 0.9370\n",
      "Epoch 27/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2613 - acc: 0.9329\n",
      "Epoch 28/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2549 - acc: 0.9350\n",
      "Epoch 29/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2629 - acc: 0.9309\n",
      "Epoch 30/30\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.2827 - acc: 0.9207\n",
      "123/123 [==============================] - 1s 4ms/step\n",
      "492/492 [==============================] - 0s 797us/step\n",
      "[CV]  shuffle=True, optim_methods=SGD, num_hidden_layers=[1, 3], l2_rate=0.01, input_num_hidden_units=3, input_dropout_rates=0.5, epochs=30, dropout_rates=0.5, batch_size=8, batch_norm=no, activation_function=relu, score=0.6016260167447532, total=  28.1s\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 3)                 242043    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 3)                 6         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 242,057\n",
      "Trainable params: 242,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "615/615 [==============================] - 3s 4ms/step - loss: 0.7459 - acc: 0.5122\n",
      "Epoch 2/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.7125 - acc: 0.6195\n",
      "Epoch 3/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.6546 - acc: 0.7220\n",
      "Epoch 4/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.6316 - acc: 0.7610\n",
      "Epoch 5/30\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 0.5810 - acc: 0.7756\n",
      "Epoch 6/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.5363 - acc: 0.8163\n",
      "Epoch 7/30\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 0.4532 - acc: 0.8650\n",
      "Epoch 8/30\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 0.4323 - acc: 0.8715\n",
      "Epoch 9/30\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 0.4056 - acc: 0.8813\n",
      "Epoch 10/30\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 0.3755 - acc: 0.8894\n",
      "Epoch 11/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.3242 - acc: 0.9285\n",
      "Epoch 12/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.3468 - acc: 0.9024\n",
      "Epoch 13/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2959 - acc: 0.9285\n",
      "Epoch 14/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.3104 - acc: 0.9187\n",
      "Epoch 15/30\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 0.2942 - acc: 0.9252\n",
      "Epoch 16/30\n",
      "615/615 [==============================] - 2s 2ms/step - loss: 0.2911 - acc: 0.9252\n",
      "Epoch 17/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2859 - acc: 0.9268\n",
      "Epoch 18/30\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 0.3041 - acc: 0.9171\n",
      "Epoch 19/30\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 0.2774 - acc: 0.9285\n",
      "Epoch 20/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2792 - acc: 0.9285\n",
      "Epoch 21/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2832 - acc: 0.9252\n",
      "Epoch 22/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2337 - acc: 0.9480\n",
      "Epoch 23/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2603 - acc: 0.9333\n",
      "Epoch 24/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2429 - acc: 0.9431\n",
      "Epoch 25/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2615 - acc: 0.9333\n",
      "Epoch 26/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2880 - acc: 0.9187\n",
      "Epoch 27/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2174 - acc: 0.9528\n",
      "Epoch 28/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2560 - acc: 0.9333\n",
      "Epoch 29/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2625 - acc: 0.9301\n",
      "Epoch 30/30\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 0.2784 - acc: 0.9203\n",
      "Best score obtained: 0.6764227644699375\n",
      "Parameters:\n",
      "\tshuffle: True\n",
      "\toptim_methods: SGD\n",
      "\tnum_hidden_layers: [1, 3]\n",
      "\tl2_rate: 0.01\n",
      "\tinput_num_hidden_units: 3\n",
      "\tinput_dropout_rates: 0.5\n",
      "\tepochs: 30\n",
      "\tdropout_rates: 0.5\n",
      "\tbatch_size: 8\n",
      "\tbatch_norm: no\n",
      "\tactivation_function: relu\n",
      "Predict uisng test data using random_search:\n",
      "150/150 [==============================] - 0s 3ms/step\n",
      "acc y_pred_random_search: 0.6266666666666667\n",
      "Evaluate the best model on training data:\n",
      "615/615 [==============================] - 1s 1ms/step\n",
      "loss :  0.12089832242184538\n",
      "acc :  0.9967479674796748\n",
      "Evaluate the best model on test data:\n",
      "150/150 [==============================] - 0s 473us/step\n",
      "loss :  0.8929051224390666\n",
      "acc :  0.6266666674613952\n"
     ]
    }
   ],
   "source": [
    "# fit to the training data\n",
    "random_search.fit(x_train, y_train)\n",
    "print('Best score obtained: {0}'.format(random_search.best_score_))\n",
    "print('Parameters:')\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print('\\t{}: {}'.format(param, value))\n",
    "\n",
    "\n",
    "# predict the random search using the test data\n",
    "print('Predict uisng test data using random_search:')\n",
    "y_pred_random_search = random_search.predict(x_test)\n",
    "acc_pred_random_search = accuracy_score(y_test, y_pred_random_search)\n",
    "print('acc y_pred_random_search:', acc_pred_random_search)\n",
    "\n",
    "\n",
    "# validator.best_estimator_ returns sklearn-wrapped version of best model.\n",
    "# validator.best_estimator_.model returns the (unwrapped) keras model\n",
    "print('Evaluate the best model on training data:')\n",
    "best_model = random_search.best_estimator_.model\n",
    "metric_names = best_model.metrics_names\n",
    "metric_values = best_model.evaluate(x_train, y_train)\n",
    "for metric, value in zip(metric_names, metric_values):\n",
    "    print(metric, ': ', value)\n",
    "\n",
    "# predict the test data using the best model obtained\n",
    "print('Evaluate the best model on test data:')\n",
    "metric_values_on_test_data = best_model.evaluate(x_test, y_test)\n",
    "for metric, value in zip(metric_names, metric_values_on_test_data):\n",
    "    print(metric, ': ', value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
